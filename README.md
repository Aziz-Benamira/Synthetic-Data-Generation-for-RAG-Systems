# Synthetic RAG Evaluation

A research project focused on generating and evaluating synthetic datasets for Retrieval-Augmented Generation (RAG) systems in specialized domains.

## Overview

This project explores methods for creating synthetic question-answer pairs to evaluate and improve RAG systems, particularly for domain-specific use cases where labeled data is scarce. The generated datasets will be published on HuggingFace.

## Objectives

- Review state-of-the-art synthetic data generation methods
- Implement and test various generation approaches
- Evaluate quality of synthetic data (coverage, relevance, diversity)
- Develop novel methods combining different techniques (graph-based, agentic, RL, active learning)
- Assess alignment between synthetic and real-world RAG performance

## Project Structure

The project is organized into several research tracks:

- **Data Generation**: Methods for creating synthetic question-answer pairs
- **Evaluation Metrics**: Frameworks for assessing RAG system performance
- **Question Typology**: Analysis of question types (single-hop, multi-hop, reasoning)
- **Benchmarking**: Testing on domain-specific corpora

## Getting Started

More details coming soon as the project develops.

## Contributing

This is a collaborative research project. Each contributor focuses on specific aspects of the pipeline.

## License

TBD
